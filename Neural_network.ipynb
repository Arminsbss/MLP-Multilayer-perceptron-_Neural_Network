{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arminsbss/MLP-Multilayer-perceptron-_Neural_Network-/blob/main/Neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAY4GvEp9XTW"
      },
      "outputs": [],
      "source": [
        "'''====================Armin SabourMoghaddam ===================='''\n",
        "#==========Import sklearn Iris data(150*4)==========\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "#==========We use 80% data for training(120) and 20% for test(30)==========\n",
        "'''\n",
        "train_data=[[Iris[0,:]],\n",
        "            [Iris[1,:]],\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            [Iris[39,:]],\n",
        "d[j]=0\n",
        "            [Iris[50,:]]\n",
        "            [Iris[51,:]],\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            [Iris[89,;]]\n",
        "d[j]=1\n",
        "            [Iris[100,:]\n",
        "            [Iris[101,:],\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            [Iris[139,:]]]\n",
        "d[j]=2\n",
        "test_data=[[Iris[40,:]]\n",
        "            [Iris[41,:]],\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            [Iris[49]]\n",
        "            [Iris[90,:]]\n",
        "            [Iris[91,:]],\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            [Iris[99,;]]\n",
        "\n",
        "            [Iris[140,:]\n",
        "            [Iris[141,:],\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            [Iris[149,:]]]\n",
        "'''\n",
        "Iris= iris.data\n",
        "Train_data=[]\n",
        "for i in range(40):\n",
        "  Train_data.append(Iris[i,:])\n",
        "# if Train_data[0][0]==Iris[0][0]:\n",
        "#   print(\"good\")\n",
        "for i in range(50,90):\n",
        "  Train_data.append(Iris[i,:])\n",
        "for i in range(100,140):\n",
        "  Train_data.append(Iris[i,:])\n",
        "Test_data=[]\n",
        "for i in range(40,50):\n",
        "  Test_data.append(Iris[i,:])\n",
        "for i in range(90,100):\n",
        "  Test_data.append(Iris[i,:])\n",
        "for i in range(140,150):\n",
        "  Test_data.append(Iris[i,:])\n",
        "\n",
        "\n",
        "# Train_data=[[1,1]] #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "INPUT_LAYER = 4   # for iris 4 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "OUTPUT_LAYER = 3   # for iris 3   @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06cWcSFa4evX"
      },
      "outputs": [],
      "source": [
        "#==========Input layer from sklean iris==========\n",
        "def input_layer(x1,x2,x3,x4):\n",
        "  # x1=5.843333333333334\n",
        "  # x2=3.0573333333333332\n",
        "  # x3=3.758\n",
        "  # x4=1.1993333333333334\n",
        "  X=[x1,x2,x3,x4]\n",
        "  return X\n",
        "# import statistics\n",
        "# arr=[]\n",
        "# for i in range (150):\n",
        "#   arr.append(Iris[i][3])\n",
        "# x=statistics.mean(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkpqupUR1cm8"
      },
      "outputs": [],
      "source": [
        "#==========function RM generates random matrix n*m(use for first weights matrix assignment)==========\n",
        "import random\n",
        "def RM(n,m): # N * M\n",
        "  Random_Matrix=[ [random.random() for i in range(m)] for i in range(n) ]\n",
        "  # Random_Matrix=[ [1 for i in range(m)] for i in range(n) ] \n",
        "  # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "  return Random_Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFNIrcOfvkIS",
        "outputId": "aa3462c8-a0e2-4483-9ee1-4f42220584a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter number of hidden layers:1\n",
            "enter number of 1th layer neurons:1\n",
            "type 1 for Linear, 2 for Sigmoid, 3 for Tanh and 4 for Relu:1\n",
            "type 1 for Linear, 2 for Sigmoid, 3 for Tanh and 4 for Relu):1\n",
            "enter learning rate:1\n"
          ]
        }
      ],
      "source": [
        "#==========assignments==========\n",
        "# hidden_layer=1\n",
        "# neurons=3\n",
        "# activation_function=4\n",
        "# learning_rate=0.5\n",
        "hidden_layer=int(input(\"enter number of hidden layers:\"))  # NUMBER OF HIDDEIN LAYERS\n",
        "\n",
        "neuron=[]   # number of perceptron in each hidden layer\n",
        "activation_functions=[] \n",
        "\n",
        "for i in range(int(hidden_layer)):\n",
        "  neuron.append(int(input(\"enter number of %dth layer neurons:\"%(i+1))))\n",
        "  activation_functions.append(int(input(\"type 1 for Linear, 2 for Sigmoid, 3 for Tanh and 4 for Relu:\")))\n",
        "activation_functions.append(int(input(\"type 1 for Linear, 2 for Sigmoid, 3 for Tanh and 4 for Relu):\")))\n",
        "\n",
        "# if activation_functions>4 or activation_functions<1:\n",
        "#   print(\"Error! Out of range!\")\n",
        "#   quit()\n",
        "learning_rate=float(input(\"enter learning rate:\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNXy9yoYNRqS"
      },
      "outputs": [],
      "source": [
        "#==========Neuron summation==========\n",
        "def neuron_sum(w,x) :\n",
        "    if len(w[0])!=len(x):\n",
        "      return -1\n",
        "    C = []\n",
        "    for i in range(len(w)) :\n",
        "        row = []        \n",
        "        for j in range(len(x[0])) :\n",
        "            result = 0\n",
        "            for k in range(len(x)) :\n",
        "                result += w[i][k] * x[k][j]\n",
        "            row.append(result)\n",
        "        C.append(row)\n",
        "    return C\n",
        "import numpy as np\n",
        "def neuron_sum2(w,x):\n",
        "  return np.dot(w,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my8wncXQ_rFD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "#==========Activation functions==========\n",
        "\n",
        "#==========Linear equal to 1==========\n",
        "def Lin(x):\n",
        "  y=[]\n",
        "  for i in x:\n",
        "    y.append(i)\n",
        "  return y\n",
        "#==========Sigmoid equal to 2==========\n",
        "def Sig(x):\n",
        "  y=[]\n",
        "  for i in x:\n",
        "    y.append(1/(1+math.exp(-i)))\n",
        "  return y\n",
        "# try:\n",
        "#     ans = Sig(-999)\n",
        "# except OverflowError:\n",
        "#     ans = float('inf')\n",
        "# ans\n",
        "#==========Tanh equal to 3==========\n",
        "def Tanh(x):\n",
        "  y=[]\n",
        "  for i in x:\n",
        "    y.append(math.tanh(i))\n",
        "  return y\n",
        "#==========Relu equal to 4==========\n",
        "def Relu(x):\n",
        "  y=[]\n",
        "  for i in x:\n",
        "    y.append(max(0,i))\n",
        "  return y\n",
        "#==========Softmax function==========\n",
        "#https://stackoverflow.com/questions/2474015/getting-the-index-of-the-returned-max-or-min-item-using-max-min-on-a-list\n",
        "# calculate the softmax of a vector\n",
        "from numpy import exp\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x)\n",
        "    index_max=max(range(len(e_x)), key=e_x.__getitem__)\n",
        "    return index_max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_activation_function(funcCode ,inputVector):\n",
        "  if funcCode==1:\n",
        "    return inputVector\n",
        "\n",
        "  elif funcCode==2:\n",
        "    return Sig(inputVector)\n",
        "\n",
        "  elif funcCode==3:\n",
        "    return Tanh(inputVector)\n",
        "\n",
        "  else:\n",
        "    return Relu(inputVector)\n"
      ],
      "metadata": {
        "id": "kukN56JlpxRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_activation_function(funcCode ,inputVector):\n",
        "  if funcCode==1:\n",
        "    for i in range(len(inputVector)):\n",
        "      inputVector[i]=1\n",
        "    return inputVector\n",
        "\n",
        "  elif funcCode==2:\n",
        "    for i in range(len(inputVector)):\n",
        "      v=compute_activation_function(2,inputVector)\n",
        "      inputVector[i]=v[i]*(1-v[i])\n",
        "    return inputVector\n",
        "\n",
        "  elif funcCode==3:\n",
        "    for i in range(len(inputVector)):\n",
        "      v=compute_activation_function(3,inputVector)\n",
        "      inputVector[i]=(1-v[i]*v[i])\n",
        "    return inputVector\n",
        "\n",
        "  else:\n",
        "    for i in range(len(inputVector)):\n",
        "      if inputVector[i]<0:\n",
        "        inputVector[i]=0\n",
        "      else:\n",
        "        inputVector[i]=1\n",
        "    return inputVector\n"
      ],
      "metadata": {
        "id": "wfpZdrvUSqdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change desierd output into onehot function\n",
        "import pandas\n",
        "Train_data_class=[]\n",
        "for i in range(40):\n",
        "  Train_data_class.append(0)#100\n",
        "for i in range(40):\n",
        "  Train_data_class.append(1)#010\n",
        "for i in range(40):\n",
        "  Train_data_class.append(2)#001\n",
        "desierd=pandas.get_dummies(Train_data_class)\n",
        "\n",
        "# desierdOutput_for_Train_data = [[2,2]] ####@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "desierdOutput_for_Train_data = desierd.to_numpy() ##@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "\n",
        "Test_data_class=[]\n",
        "for i in range(10):\n",
        "  Test_data_class.append(0)\n",
        "for i in range(10):\n",
        "  Test_data_class.append(1)\n",
        "for i in range(10):\n",
        "  Test_data_class.append(2)\n",
        "desierd=pandas.get_dummies(Test_data_class)\n",
        "desierdOutput_for_Test_data = desierd.to_numpy()"
      ],
      "metadata": {
        "id": "lFiM_IHWdVOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XC1670n9jOH"
      },
      "outputs": [],
      "source": [
        "#==========Neural Network==========\n",
        "#==========Bias variance trade_off==========\n",
        "#==========Learning rate is first learning_rate then 1/2*learning_rate and at last 1/8==========\n",
        "\n",
        "\n",
        "#==========first we generate random weights==========\n",
        "W=[]\n",
        "W.append(RM(INPUT_LAYER,neuron[0]))\n",
        "for i2 in range(1,hidden_layer):\n",
        "  W.append(RM(neuron[i2-1],neuron[i2]))\n",
        "W.append(RM(neuron[hidden_layer-1],OUTPUT_LAYER))\n",
        "import copy\n",
        "W_new=copy.deepcopy(W) # for updating weights\n",
        "# flag=len(W[0])*len(W[0][0])\n",
        "\n",
        "delta=[]\n",
        "layerOutput=[]\n",
        "layerNet=[]\n",
        "epoch=50\n",
        "while epoch>0:\n",
        "  epoch-=1\n",
        "  for i1 in range(len(Train_data)):\n",
        "  # for i1 in range(1):\n",
        "    layerOutput=[]\n",
        "    layerNet=[]\n",
        "    delta=[]\n",
        "    layerNet.append(Train_data[i1])\n",
        "    layerOutput.append(layerNet[-1])\n",
        "    for i3 in range(hidden_layer):\n",
        "      layerNet.append(neuron_sum2(layerOutput[i3],W[i3]))\n",
        "      layerOutput.append(compute_activation_function(activation_functions[i3],layerNet[-1]))\n",
        "    layerNet.append(neuron_sum2(layerOutput[hidden_layer],W[hidden_layer]))\n",
        "    layerOutput.append(compute_activation_function(activation_functions[hidden_layer],layerNet[-1]))\n",
        "\n",
        "    # delta for outputlayer(with 3 neuron):\n",
        "    delta.append(derivative_activation_function(activation_functions[-1],layerNet[-1]))\n",
        "    for i in range(len(delta)):\n",
        "      delta[i]*=desierdOutput_for_Train_data[i1][i]-layerOutput[-1][i]\n",
        "    # print(delta)\n",
        "\n",
        "    # delta.append(derivative_activation_function(activation_functions[-1],layerNet[-1][0])*(desierdOutput_for_Test_data[i1][0]-layerOutput[-1][0]))\n",
        "    # delta.append(derivative_activation_function(activation_functions[-1],layerNet[-1][1])*(desierdOutput_for_Test_data[i1][1]-layerOutput[-1][1]))\n",
        "    # delta.append(derivative_activation_function(activation_functions[-1],layerNet[-1][2])*(desierdOutput_for_Test_data[i1][2]-layerOutput[-1][2]))\n",
        "\n",
        "    # delta.append(derivative_activation_function(activation_functions[-1],layerNet[-1]))\n",
        "    # delta=neuron_sum2(delta,(desierdOutput_for_Test_data[i1]-layerOutput[-1]))\n",
        "\n",
        "    # delta[-1][0]*=(0-layerOutput[-1][0])\n",
        "    # delta[-1][1]*=(1-layerOutput[-1][1])\n",
        "    # delta[-1][2]*=(2-layerOutput[-1][2])\n",
        "\n",
        "  #delta for hidden layers:\n",
        "    #derivatives of functions for hidden layers:\n",
        "    for i4 in range(len(layerOutput)-2,-1,-1): \n",
        "      delta.insert(0,derivative_activation_function(activation_functions[i4-1],layerNet[i4]))\n",
        "    \n",
        "    # for calculate delta we need to multiply delta*(sigma of front delta*weights)\n",
        "    for i in range(len(delta)-2,-1,-1): #i=2,1,0\n",
        "      for j in range(len(delta[i])): #j=0,1,2,3&j=0,1,2&j=0,1,2,3\n",
        "        sigma=0\n",
        "        for k in range(len(delta[i+1])):#k=0,1,2\n",
        "          sigma+=delta[i+1][k]*W[i][j][k]\n",
        "          W_new[i][j][k]+=learning_rate*delta[i+1][k]*layerOutput[i][j]\n",
        "          # if W_new[i][j][k]==0:\n",
        "          #   print(delta[i+1][k],\"---\",layerOutput[i][j])\n",
        "        delta[i][j]*=sigma\n",
        "    \n",
        "\n",
        "    W=copy.deepcopy(W_new) # for updating weights    \n",
        "    \n",
        "    \n",
        "    # for k in range(len(W_new)):\n",
        "    #   for i in range(len(W_new[k])):\n",
        "    #     for j in range(len(W_new[k][i])):\n",
        "    #       W_new[k][i][j]+=learning_rate*delta[k+1][j]W[k-1][]\n",
        "\n",
        "    # delta for outputlayer(with 3 neuron):\n",
        "    # # update weights for output layer\n",
        "    # for i4 in range(len(W[-1])):\n",
        "    #   for j4 in range(len(W[-1][i4])):\n",
        "    #     W[-1][i4][j4]+=learning_rate*delta[-1][j4]*layerOutput[-2][i4]\n",
        "    # sum_value=0\n",
        "    # for i4 in range(len(W),0,-1):\n",
        "    #   for jj in range(len(W[i4-1])): #i4-1 because index start with 0\n",
        "    #     for kk in range(len(W[i4-1][jj])):\n",
        "    #       sum_value+=W[i4-1][jj][kk]*delta[-1][kk]\n",
        "    #     delta.insert(0,[layerOutput[i4-1][jj]*(1-layerOutput[i4-1][jj])*sum_value])\n",
        "    \n",
        "    # for i4 in range(len(W),0,-1):\n",
        "    #   for jj in range(len(W[i4-1])):\n",
        "    #     for kk in range(len(W[i4-1][jj])):\n",
        "    #       W[i4-1][jj][kk]+=learning_rate*delta[i4-1][jj]*layerOutput[i4-1][jj]\n",
        "    # # for i4 in range(hidden_layer,-1,-1):\n",
        "    # #   for i in range(len(W[hidden_layer]),-1,-1):\n",
        "    # #     for j in range(len(W[hidden_layer][i-1]),-1,-1):\n",
        "    # #       if i4==hidden_layer : #delta for outputlayer\n",
        "    # #         delta.append((softmax(layerOutput[i][j]))*(1-softmax(layerOutput[i][j]))*(d-softmax(layerOutput[-])))\n",
        "    # #         delta_value=delta[-1]\n",
        "    # #         W[hidden_layer][i][j]+=(learning_rate*delta_value*layerOutput[i][j])\n",
        "    # #       else: #delta for hidden layers\n",
        "    # #         delta.append((layerOutput[i][j])*(1-layerOutput[i][j])*sum(neuron_sum2(W[i4][i],delta)))\n",
        "    # #         delta_value=delta[-1]\n",
        "    # #         W[hidden_layer][i][j]+=(learning_rate*delta_value*layerOutput[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========Run on Train_data==========\n",
        "counter=0\n",
        "for i1 in range(len(Train_data)):\n",
        "  layerOutput=[]\n",
        "  layerNet=[]\n",
        "  layerNet.append(Train_data[i1])\n",
        "  layerOutput.append(layerNet[-1])\n",
        "  for i3 in range(hidden_layer):\n",
        "    layerNet.append(neuron_sum2(layerOutput[i3],W[i3]))\n",
        "    layerOutput.append(compute_activation_function(activation_functions[i3],layerNet[-1]))\n",
        "  layerNet.append(neuron_sum2(layerOutput[hidden_layer],W[hidden_layer]))\n",
        "  layerOutput.append(compute_activation_function(activation_functions[hidden_layer],layerNet[-1]))\n",
        "\n",
        "  if i1<51:\n",
        "    if softmax(layerOutput[-1])!=0:# first classification\n",
        "      # print(\"-----first class-----\")\n",
        "      # print(\"error\")\n",
        "      counter+=1\n",
        "  elif i1>50 and i1<101:\n",
        "    if softmax(layerOutput[-1])!=1:#secound classification\n",
        "      # print(\"-----secound class-----\")\n",
        "      # print(\"error\")\n",
        "      counter+=1\n",
        "  else:\n",
        "    if softmax(layerOutput[-1])!=2:#third classification\n",
        "      # print(\"-----third class-----\")\n",
        "      # print(\"error\")\n",
        "      counter+=1\n",
        "print(\"the number of errors:\",counter)\n",
        "accuracy=(120-counter)*100/120\n",
        "print(\"Accuracy:\",accuracy)   "
      ],
      "metadata": {
        "id": "DPXclGXaK3yZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30659a85-2833-4e2f-f13a-095c230fa5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of errors: 69\n",
            "Accuracy: 42.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========Run on Test_data==========\n",
        "counter=0\n",
        "for i1 in range(len(Test_data)):\n",
        "  layerOutput=[]\n",
        "  layerNet=[]\n",
        "  layerNet.append(Test_data[i1])\n",
        "  layerOutput.append(layerNet[-1])\n",
        "  for i3 in range(hidden_layer):\n",
        "    layerNet.append(neuron_sum2(layerOutput[i3],W[i3]))\n",
        "    layerOutput.append(compute_activation_function(activation_functions[i3],layerNet[-1]))\n",
        "  layerNet.append(neuron_sum2(layerOutput[hidden_layer],W[hidden_layer]))\n",
        "  layerOutput.append(compute_activation_function(activation_functions[hidden_layer],layerNet[-1]))\n",
        "\n",
        "  if i1<11:\n",
        "    if softmax(layerOutput[-1])!=0:# first classification\n",
        "      # print(\"-----first class-----\")\n",
        "      # print(\"error\")\n",
        "      counter+=1\n",
        "  elif i1>20 and i1<31:\n",
        "    if softmax(layerOutput[-1])!=1:#secound classification\n",
        "      # print(\"-----secound class-----\")\n",
        "      # print(\"error\")\n",
        "      counter+=1\n",
        "  else:\n",
        "    if softmax(layerOutput[-1])!=2:#third classification\n",
        "      # print(\"-----third class-----\")\n",
        "      # print(\"error\")\n",
        "      counter+=1\n",
        "print(\"the number of errors:\",counter)\n",
        "accuracy=(30-counter)*100/30\n",
        "print(\"Accuracy:\",accuracy)   "
      ],
      "metadata": {
        "id": "3vkG34o7Q8js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fcb19e-d262-4cbb-fe29-46bb594d874c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of errors: 19\n",
            "Accuracy: 36.666666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "amBB6siZJCNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Neural network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZr2JhUhvGaB680pHM1lGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}